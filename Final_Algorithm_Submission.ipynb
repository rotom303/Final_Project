{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZk8kzDGQzXceKO8CEEEAv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rotom303/Final_Project/blob/main/Final_Algorithm_Submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run all cells in this notebook. Before running, ensure that the current working directory (i.e. the directory where the drive is mounted) contains the 'models' folder and the 'raw_training_dat.mat' file. The code loads the models from the 'models' folder and uses the 'raw_training_data.mat' file to normalize the testing data. It assumes that the 'models' folder and the 'raw_training_data.mat' and located in its current working directory, along with the 'truetest_data.mat' file."
      ],
      "metadata": {
        "id": "Db_krGHe3TB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The code outputs predictions.mat, which contains a variable predicted_dg, which contains our predictions on the hidden test set."
      ],
      "metadata": {
        "id": "_uZE7bDtCJi2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f5UPPlWrQLb"
      },
      "outputs": [],
      "source": [
        "#Set up the notebook environment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import pearsonr\n",
        "from scipy import signal as sig\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.svm import SVR\n",
        "from numpy.linalg import inv\n",
        "\n",
        "NUM_FINGERS = 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') \n",
        "\n",
        "#%cd drive/MyDrive/Penn/'Spring 2023'/'BE 5210'/Project/Part_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LifzWDU70Icl",
        "outputId": "766d5c88-0872-4082-a335-5cc3b7752bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'content/drive/MyDrive/Penn/Spring 2023/BE 5210/Project/Part_2'\n",
            "/root\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n",
        "\n",
        "proj_data=sio.loadmat('raw_training_data.mat')\n",
        "\n",
        "#splitting into testing and training sets\n",
        "train_data_proportion = 0.7\n",
        "#patient 1\n",
        "\n",
        "random.seed(123)\n",
        "\n",
        "#glove\n",
        "total_p1_glove=proj_data['train_dg'][0][0]\n",
        "training_rows = list(range(0, int(len(total_p1_glove)*train_data_proportion)))\n",
        "training_p1_glove=total_p1_glove[training_rows,:] #60% of samples in training\n",
        "testing_p1_glove=np.delete(total_p1_glove, training_rows, axis=0) #40% of samples in testing\n",
        "\n",
        "#ecog\n",
        "total_p1_ecog=proj_data['train_ecog'][0][0]\n",
        "# total_p1_ecog=np.delete(total_p1_ecog, 54, 1)\n",
        "training_rows = list(range(0, int(len(total_p1_ecog)*train_data_proportion)))\n",
        "training_p1_ecog=total_p1_ecog[training_rows,:] #60% of samples in training\n",
        "testing_p1_ecog=np.delete(total_p1_ecog, training_rows, axis=0) #60% of samples in training\n",
        "\n",
        "#patient 2\n",
        "\n",
        "#glove\n",
        "total_p2_glove=proj_data['train_dg'][1][0]\n",
        "training_rows = list(range(0, int(len(total_p2_glove)*train_data_proportion)))\n",
        "training_p2_glove=total_p2_glove[training_rows,:] #60% of samples in training\n",
        "testing_p2_glove=np.delete(total_p2_glove, training_rows, axis=0) #60% of samples in training\n",
        "#ecog\n",
        "total_p2_ecog=proj_data['train_ecog'][1][0]\n",
        "# total_p2_ecog=np.delete(total_p2_ecog, [20, 37], 1)\n",
        "training_rows = list(range(0, int(len(total_p2_ecog)*train_data_proportion)))\n",
        "training_p2_ecog=total_p2_ecog[training_rows,:] #60% of samples in training\n",
        "testing_p2_ecog=np.delete(total_p2_ecog, training_rows, axis=0) #60% of samples in training\n",
        "\n",
        "#patient 3\n",
        "\n",
        "#glove\n",
        "total_p3_glove=proj_data['train_dg'][2][0]\n",
        "training_rows = list(range(0, int(len(total_p3_glove)*train_data_proportion)))\n",
        "training_p3_glove=total_p3_glove[training_rows,:] #60% of samples in training\n",
        "testing_p3_glove=np.delete(total_p3_glove, training_rows, axis=0) #60% of samples in training\n",
        "#ecog\n",
        "total_p3_ecog=proj_data['train_ecog'][2][0]\n",
        "training_rows = list(range(0, int(len(total_p3_ecog)*train_data_proportion)))\n",
        "training_p3_ecog=total_p3_ecog[training_rows,:] #60% of samples in training\n",
        "testing_p3_ecog=np.delete(total_p3_ecog, training_rows, axis=0) #60% of samples in training"
      ],
      "metadata": {
        "id": "caLeZW8duHyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_data(raw_eeg, fs=1000, cutoffs=[75, 200]):\n",
        "  \"\"\"\n",
        "  Write a filter function to clean underlying data.\n",
        "  Filter type and parameters are up to you. Points will be awarded for reasonable filter type, parameters and application.\n",
        "  Please note there are many acceptable answers, but make sure you aren't throwing out crucial data or adversly\n",
        "  distorting the underlying data!\n",
        "\n",
        "  Input: \n",
        "    raw_eeg (samples x channels): the raw signal\n",
        "    fs: the sampling rate (1000 for this dataset)\n",
        "  Output: \n",
        "    clean_data (samples x channels): the filtered signal\n",
        "  \"\"\"\n",
        "  number_of_channels = np.shape(raw_eeg)[1] #number of channels\n",
        "  filteredData = np.zeros(np.shape(raw_eeg)); #filtered data output\n",
        "\n",
        "  #butterworth filter of 4th order\n",
        "  sos = sig.butter(4, cutoffs, 'bandpass', analog=False, fs=fs, output='sos'); # returns filter coefficients\n",
        "\n",
        "  #for each channel\n",
        "  for chanInd in np.arange(number_of_channels):\n",
        "    # subtract mean from each datapoint\n",
        "    currFilt = raw_eeg[:, chanInd] - np.mean(raw_eeg[:, chanInd]);\n",
        "    currFilt = sig.sosfiltfilt(sos, currFilt) # forward-backward digital filter using cascaded second-order sections                                \n",
        "    filteredData[:, chanInd] = currFilt\n",
        "  return filteredData"
      ],
      "metadata": {
        "id": "ppnQJzxJrfY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NumWins(x, winLen, winDisp, fs):\n",
        "\n",
        "    # Calculate the length of the signal in samples\n",
        "    xLen = len(x)\n",
        "    # Calculate the number of windows\n",
        "    windows = ((len(x)/fs)/ winDisp) - (winLen / winDisp) + (winDisp / winDisp) - ((((len(x)/fs) - winLen + winDisp) % winDisp)/winDisp)\n",
        "    #windows = ((len(x)/fs)/ winDisp) - (winLen / winDisp) + (winDisp / winDisp)\n",
        "    return int(windows)"
      ],
      "metadata": {
        "id": "C2TXbUGlrhFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def line_length(signal):\n",
        "    return np.sum(np.absolute(np.ediff1d(signal))) \n",
        "\n",
        "def zero_crossings(x):\n",
        "  xbar=np.mean(x)\n",
        "  number_of_crossings = 0\n",
        "  for i in range(1,len(x)):\n",
        "    if (x[i-1]-xbar) > 0 and (x[i]-xbar) < 0:\n",
        "      number_of_crossings += 1\n",
        "    elif (x[i-1]-xbar) < 0 and (x[i]-xbar) > 0:\n",
        "      number_of_crossings += 1\n",
        "  return number_of_crossings\n",
        "\n",
        "def energy(signal):\n",
        "  return np.sum(np.square(signal))\n",
        "\n",
        "def area(signal):\n",
        "  return np.sum(np.absolute(signal))\n",
        "\n",
        "def time_avg(signal):\n",
        "  return np.mean(signal)\n",
        "\n",
        "from numpy.fft import fft, ifft\n",
        "\n",
        "def band_power(signal,fs,low,high):\n",
        "  X = fft(signal)\n",
        "  N = len(X)\n",
        "  n = np.arange(N)\n",
        "  T = N/fs #sampling rate=1000\n",
        "  freq = n/T \n",
        "  power_spectrum=np.abs(X)\n",
        "\n",
        "  # Find values in frequency vector corresponding to input band\n",
        "  index_band = np.logical_and(freq >= low, freq <= high)\n",
        "  #average frequency domain magnitude\n",
        "  avg_mag=np.mean(power_spectrum[index_band])\n",
        "\n",
        "  return avg_mag"
      ],
      "metadata": {
        "id": "EnAzwuvxriVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features(filtered_window, fs=1000):\n",
        "  \"\"\"\n",
        "    Write a function that calculates features for a given filtered window. \n",
        "    Feel free to use features you have seen before in this class, features that\n",
        "    have been used in the literature, or design your own!\n",
        "\n",
        "    Input: \n",
        "      filtered_window (window_samples x channels): the window of the filtered ecog signal \n",
        "      fs: sampling rate\n",
        "    Output:\n",
        "      features (channels x num_features): the features calculated on each channel for the window\n",
        "  \"\"\"\n",
        "  #print(np.shape(filtered_window))\n",
        "  [window_samples,num_channels]=np.shape(filtered_window)\n",
        "\n",
        "  features=np.empty((num_channels,5))\n",
        "\n",
        "  for cc in range(num_channels):\n",
        "\n",
        "    curr_window=filtered_window[:,cc] \n",
        "\n",
        "    #features[cc,0]=line_length(curr_window)\n",
        "    #features[cc,0]=area(curr_window)\n",
        "    features[cc,0]=energy(curr_window) # Energy in signal window\n",
        "    #features[cc,3]=zero_crossings(curr_window)\n",
        "\n",
        "    features[cc,1]=band_power(curr_window,1000,8,12) # average frequency-domain magnitude in alpha frequency band\n",
        "    #features[cc,2]=band_power(curr_window,1000,13,30) # average frequency-domain magnitude in beta frequency band\n",
        "    features[cc,2]=band_power(curr_window,1000,75,95) # average frequency-domain magnitude in low gamma frequency band\n",
        "    features[cc,3]=band_power(curr_window,1000,96,115) # average frequency-domain magnitude in high gamma frequency band\n",
        "    features[cc,4]=time_avg(curr_window) #Average time-domain voltage\n",
        "\n",
        "  return features"
      ],
      "metadata": {
        "id": "XWt_FNrNrjqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_windowed_feats(raw_ecog, fs, window_length, window_overlap):\n",
        "  \"\"\"\n",
        "    Write a function which processes data through the steps of filtering and\n",
        "    feature calculation and returns features. Points will be awarded for completing\n",
        "    each step appropriately (note that if one of the functions you call within this script\n",
        "    returns a bad output, you won't be double penalized). Note that you will need\n",
        "    to run the filter_data and get_features functions within this function. \n",
        "\n",
        "    Inputs:\n",
        "      raw_eeg (samples x channels): the raw signal\n",
        "      fs: the sampling rate (1000 for this dataset)\n",
        "      window_length: the window's length\n",
        "      window_overlap: the window's overlap\n",
        "    Output: \n",
        "      all_feats (num_windows x (channels x features)): the features for each channel for each time window\n",
        "        note that this is a 2D array. \n",
        "  \"\"\"\n",
        "  clean_data=filter_data(raw_ecog, fs)\n",
        "  [num_samples,num_channels]=np.shape(clean_data)\n",
        "\n",
        "  num_windows = NumWins(clean_data[:,0], window_length,window_overlap, fs) #calculate number of windows and remaining time\n",
        "\n",
        "  #convert everything to units of samples\n",
        "  num_samples = len(clean_data)\n",
        "  winLen_samples=round(window_length*fs) #window length in samples\n",
        "  winDisp_samples=round(window_overlap*fs) #window displacement in samples\n",
        "  \n",
        "  # empty list to store features\n",
        "  feature_vector = []\n",
        "  # The end of the first window is the last sample\n",
        "  window_end = num_samples\n",
        "  # List to store tuples containing start and end indices for each slice\n",
        "\n",
        "  for i in range(round(num_windows)):\n",
        "\n",
        "    # Compute the start of the window using the end of the window\n",
        "    window_start = window_end - winLen_samples\n",
        "    # If the number of samples left is not sufficient, break out of the loop\n",
        "    if window_start < 0:\n",
        "        break\n",
        "    \n",
        "    signal_window=clean_data[window_start:window_end,:] #signal in that window\n",
        "    \n",
        "    # store feature of that window\n",
        "    feature_vector.append(get_features(signal_window, fs).flatten('F'))\n",
        "\n",
        "    #set new ending index of window\n",
        "    window_end -= winDisp_samples \n",
        "  \n",
        "  # Reverse at the end since we are looking at the windows backwards\n",
        "  feature_vector.reverse()\n",
        "\n",
        "\n",
        "  return feature_vector"
      ],
      "metadata": {
        "id": "n15yhgt2rk51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_R_matrix(features, N_wind):\n",
        "  \"\"\"\n",
        "  Write a function to calculate the R matrix\n",
        "\n",
        "  Input:\n",
        "    features (samples (number of windows in the signal) x channels x features): \n",
        "      the features you calculated using get_windowed_feats\n",
        "    N_wind: number of windows to use in the R matrix\n",
        "\n",
        "  Output:\n",
        "    R (samples x (N_wind*channels*features))\n",
        "  \"\"\"\n",
        "  padded_features = np.copy(features)\n",
        "  for i in list(range(N_wind-2, -1, -1)):\n",
        "      a = features[i]\n",
        "      padded_features = np.vstack([a, padded_features])\n",
        "  samples = len(features)   # number of rows = number of windows\n",
        "\n",
        "  R = np.zeros((samples, 1+(N_wind*len(features[0,:]))))  # len(features[0,:]) = (num of features)*(num of channels)\n",
        "  lst = np.array(list(range(1, 1+N_wind)))\n",
        "  R[:, 0] = 1\n",
        "\n",
        "  for i in range(len(features[0,:])):   # goes thru each column of the features matrix\n",
        "    for j in range(len(lst)):\n",
        "        x = lst[j]\n",
        "        R[:, x] = padded_features[j : (len(padded_features)-(N_wind-1-j)), i]\n",
        "    lst = lst + N_wind\n",
        "  return R"
      ],
      "metadata": {
        "id": "G38dhfxdrmTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_train_test(feature_matrix,test_feature_matrix):\n",
        "\n",
        "  [windows,feats]=np.shape(feature_matrix)\n",
        "  [windows_test,feats_test]=np.shape(test_feature_matrix)\n",
        "  normFeats=np.empty((windows,feats))\n",
        "  testnormFeats=np.empty((windows_test,feats_test))\n",
        "\n",
        "  for i in range(feats):\n",
        "      curr_train = feature_matrix[:,i]\n",
        "      mean_train=np.mean(curr_train)\n",
        "      std_train=np.std(curr_train)\n",
        "      normFeats[:,i]=(curr_train-mean_train)/std_train\n",
        "      curr_test = test_feature_matrix[:,i]\n",
        "      mean_test=np.mean(curr_test)\n",
        "      std_test=np.std(curr_test)\n",
        "      testnormFeats[:,i]=(curr_test-mean_train)/std_train #normalize test features using training mean/std\n",
        "\n",
        "  return normFeats,testnormFeats"
      ],
      "metadata": {
        "id": "axQ7Rtr9rokd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def smooth(input, smoothSize):\n",
        "  toConv = sig.gaussian(smoothSize, 3)/smoothSize;\n",
        "  return np.convolve(input, toConv, mode='same')"
      ],
      "metadata": {
        "id": "aFgtTaRErvLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading In truetest_data.mat"
      ],
      "metadata": {
        "id": "18RP8o74r4Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "truetest_data = scipy.io.loadmat('truetest_data.mat')['truetest_data']\n",
        "p1_truetest_ecog=truetest_data[0][0]\n",
        "p2_truetest_ecog=truetest_data[1][0]\n",
        "p3_truetest_ecog=truetest_data[2][0]\n",
        "p1_ecog=p1_truetest_ecog[:,np.arange(62)!=54] #remove channel 55\n",
        "p2_ecog=p2_truetest_ecog[:,np.logical_and(np.arange(48)!=20,np.arange(48)!=37)] #remove channel 21 and 38\n",
        "p3_ecog=p3_truetest_ecog"
      ],
      "metadata": {
        "id": "rdEhUtasr3TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_length=100e-3\n",
        "window_overlap=50e-3\n",
        "fs=1000\n",
        "\n",
        "p1_Feats_l=get_windowed_feats(p1_truetest_ecog, fs, window_length, window_overlap)\n",
        "p2_Feats_l=get_windowed_feats(p2_truetest_ecog, fs, window_length, window_overlap)\n",
        "p3_Feats_l=get_windowed_feats(p3_truetest_ecog, fs, window_length, window_overlap)\n",
        "p1_Feats_l=np.array(p1_Feats_l)\n",
        "p2_Feats_l=np.array(p2_Feats_l)\n",
        "p3_Feats_l=np.array(p3_Feats_l)"
      ],
      "metadata": {
        "id": "UWfXJlMusWAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1_allFeats = get_windowed_feats(total_p1_ecog, fs, window_length, window_overlap)\n",
        "p1_allFeats = np.array(p1_allFeats)\n",
        "p2_allFeats = get_windowed_feats(total_p2_ecog, fs, window_length, window_overlap)\n",
        "p2_allFeats = np.array(p2_allFeats)\n",
        "p3_allFeats = get_windowed_feats(total_p3_ecog, fs, window_length, window_overlap)\n",
        "p3_allFeats = np.array(p3_allFeats)"
      ],
      "metadata": {
        "id": "muiNftt-uB3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1_normalized_all, p1_normalized_leader = standardize_train_test(p1_allFeats, p1_Feats_l)\n",
        "p2_normalized_all, p2_normalized_leader = standardize_train_test(p2_allFeats, p2_Feats_l)\n",
        "p3_normalized_all, p3_normalized_leader = standardize_train_test(p3_allFeats, p3_Feats_l)\n",
        "R_p1_leader=create_R_matrix(p1_normalized_leader,3)\n",
        "R_p2_leader=create_R_matrix(p2_normalized_leader,3)\n",
        "R_p3_leader=create_R_matrix(p3_normalized_leader,3)"
      ],
      "metadata": {
        "id": "lnWOtLe2snaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples,channels=np.shape(p1_truetest_ecog)\n",
        "truetest_pred_3=np.zeros((samples, NUM_FINGERS))\n",
        "truetest_pred_2=np.zeros((samples, NUM_FINGERS))\n",
        "truetest_pred_1=np.zeros((samples, NUM_FINGERS))"
      ],
      "metadata": {
        "id": "olCYcfv1udbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load In Models"
      ],
      "metadata": {
        "id": "QxrRf4j0vAMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('models/p1_f1.pkl', 'rb') as f:\n",
        "    fin_p1_f1 = pickle.load(f)\n",
        "\n",
        "with open('models/p1_f2.pkl', 'rb') as f:\n",
        "    fin_p1_f2 = pickle.load(f)\n",
        "\n",
        "with open('models/p1_f3.pkl', 'rb') as f:\n",
        "    fin_p1_f3 = pickle.load(f)\n",
        "\n",
        "with open('models/p1_f4.pkl', 'rb') as f:\n",
        "    fin_p1_f4 = pickle.load(f)\n",
        "\n",
        "with open('models/p1_f5.pkl', 'rb') as f:\n",
        "    fin_p1_f5 = pickle.load(f)\n",
        "\n",
        "with open('models/p2_f1.pkl', 'rb') as f:\n",
        "    fin_p2_f1 = pickle.load(f)\n",
        "\n",
        "with open('models/p2_f2.pkl', 'rb') as f:\n",
        "    fin_p2_f2 = pickle.load(f)\n",
        "\n",
        "with open('models/p2_f3.pkl', 'rb') as f:\n",
        "    fin_p2_f3 = pickle.load(f)\n",
        "\n",
        "with open('models/p2_f4.pkl', 'rb') as f:\n",
        "    fin_p2_f4 = pickle.load(f)\n",
        "\n",
        "with open('models/p2_f5.pkl', 'rb') as f:\n",
        "    fin_p2_f5 = pickle.load(f)\n",
        "\n",
        "with open('models/p3_f1.pkl', 'rb') as f:\n",
        "    fin_p3_f1 = pickle.load(f)\n",
        "\n",
        "with open('models/p3_f2.pkl', 'rb') as f:\n",
        "    fin_p3_f2 = pickle.load(f)\n",
        "\n",
        "with open('models/p3_f3.pkl', 'rb') as f:\n",
        "    fin_p3_f3 = pickle.load(f)\n",
        "\n",
        "with open('models/p3_f4.pkl', 'rb') as f:\n",
        "    fin_p3_f4 = pickle.load(f)\n",
        "\n",
        "with open('models/p3_f5.pkl', 'rb') as f:\n",
        "    fin_p3_f5 = pickle.load(f)"
      ],
      "metadata": {
        "id": "xAWDwcbhukEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin_model_list = [[fin_p1_f1, fin_p1_f2, fin_p1_f3, fin_p1_f4, fin_p1_f5],\n",
        "              [fin_p2_f1, fin_p2_f2, fin_p2_f3, fin_p2_f4, fin_p2_f5],\n",
        "              [fin_p3_f1, fin_p3_f2, fin_p3_f3, fin_p3_f4, fin_p3_f5]]"
      ],
      "metadata": {
        "id": "mPGzVmzLzYvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict and Interpolate"
      ],
      "metadata": {
        "id": "IgBVsqTcy2Fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.interpolate import CubicSpline\n",
        "\n",
        "for i in range(5):\n",
        "    print(\"i =\", i)\n",
        "    # Patient 1\n",
        "    y_pred_test3 = fin_model_list[2][i].predict(R_p3_leader)\n",
        "    smoothed3=smooth(y_pred_test3,20)\n",
        "    cs = CubicSpline(np.arange(smoothed3.shape[0]), smoothed3)\n",
        "    new_x = np.linspace(0, smoothed3.shape[0]-1, p3_truetest_ecog.shape[0])\n",
        "    totalPrediction3 = cs(new_x)\n",
        "    truetest_pred_3[:,i]=totalPrediction3\n",
        "    \n",
        "    # Patient 2\n",
        "    y_pred_test2 = fin_model_list[1][i].predict(R_p2_leader)\n",
        "    smoothed2=smooth(y_pred_test2,20)\n",
        "    cs = CubicSpline(np.arange(smoothed2.shape[0]), smoothed2)\n",
        "    new_x = np.linspace(0, smoothed2.shape[0]-1, p2_truetest_ecog.shape[0])\n",
        "    totalPrediction2 = cs(new_x)\n",
        "    truetest_pred_2[:,i]=totalPrediction2\n",
        "\n",
        "    # Patient 1\n",
        "    y_pred_test1 = fin_model_list[0][i].predict(R_p1_leader)\n",
        "    smoothed1=smooth(y_pred_test1,20)\n",
        "    cs = CubicSpline(np.arange(smoothed1.shape[0]), smoothed1)\n",
        "    new_x = np.linspace(0, smoothed1.shape[0]-1, p1_truetest_ecog.shape[0])\n",
        "    totalPrediction1 = cs(new_x)\n",
        "    truetest_pred_1[:,i]=totalPrediction1"
      ],
      "metadata": {
        "id": "4oJGDigVy1mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.io import savemat\n",
        "\n",
        "predictions = np.zeros((3,1), dtype=object)\n",
        "predictions[0,0] = truetest_pred_1\n",
        "predictions[1,0] = truetest_pred_2\n",
        "predictions[2,0] = truetest_pred_3\n",
        "\n",
        "#save the array using the right format\n",
        "savemat('predictions.mat', {'predicted_dg':predictions})"
      ],
      "metadata": {
        "id": "hm5zv5YBzjmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
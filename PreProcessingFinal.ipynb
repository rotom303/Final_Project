{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzaH7IspMhyX4YQEn9Cokx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rotom303/Final_Project/blob/main/PreProcessingFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finger Flexion Final Project\n",
        "Developed by\n",
        "\n",
        "Alexander Byrd, Aakash Jajoo, Chaoyi Cheng\n",
        "\n"
      ],
      "metadata": {
        "id": "NkOTH7Mtn2am"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Setup"
      ],
      "metadata": {
        "id": "8v4jHpYSoEnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2ragnF2mobl",
        "outputId": "efb4f3ff-1c67-4079-c2f7-f14a1996ab38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: deepdiff in /usr/local/lib/python3.9/dist-packages (6.3.0)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /usr/local/lib/python3.9/dist-packages (from deepdiff) (4.1.0)\n"
          ]
        }
      ],
      "source": [
        "#Set up the notebook environment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from scipy.stats import pearsonr\n",
        "from scipy import signal as sig\n",
        "from scipy.io import loadmat, savemat\n",
        "from scipy.fft import fft, fftfreq\n",
        "import sklearn\n",
        "from numpy.linalg import inv\n",
        "\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "from google.colab import auth\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "!pip install deepdiff\n",
        "from deepdiff import DeepDiff\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**File Directory:**\n"
      ],
      "metadata": {
        "id": "btuAY3-7oP1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Brain_Computer_Interfaces/Final_Project/\n",
        "\n",
        "# auth.authenticate_user()\n",
        "# creds, _ = default()\n",
        "# gc = gspread.authorize(creds)\n",
        "\n",
        "proj_data = loadmat('raw_training_data.mat')\n",
        "leaderboard_data = loadmat('leaderboard_data.mat')\n",
        "fs = 1000 # Sampling frequency of the signals\n",
        "numPatients = 3 # This is just to increase reusability."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sjs6zAZocE_",
        "outputId": "44433cdf-0625-4c8e-8cea-fc0e414780ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Brain_Computer_Interfaces/Final_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter Design\n"
      ],
      "metadata": {
        "id": "eRSs2D-zoqGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Filter parameters:\n",
        "\n",
        "fc_passband: a list in form [f1,f2]. f1 and f2 are the corner frequencies of\n",
        "  a bandpass filter (-3dB attenuation at f1 & f2). Frequencies in between f1 and \n",
        "  f2 are kept (called the pass band) while others outside that range are \n",
        "  attenuated (called the rejection band). Units = Hz \n",
        "\n",
        "order: The order of the bandpass filter used. Increasing the order of a filter\n",
        "  makes the transition between the pass band and the rejection band sharper. In\n",
        "  the case of a Butterworth filter, increasing order means the pass band stays\n",
        "  flat closer to f1 and f2. Increasing the order too much will reduce filter \n",
        "  stability and can result in ripples in the pass band or other unpredictable\n",
        "  behaviour. 4th to 6th Order filters tend to be the best compromise.  \n",
        "\n",
        "applyNotch: a Boolean for whether or not to apply a notch filter. A Notch filter\n",
        "  Removes noise at precise frequencies. It is useful for removing artifacts\n",
        "  from power supplies, which are usually at harmonics of 60Hz.   \n",
        "\n",
        "f_notch: Selects the frequencies which are removed with a Notch Filter. It must\n",
        "  be a list, even if it just has 1 element. Units = Hz.\n",
        "\n",
        "Q: is the quality factor of the notch filter. A low Q will also attenuate\n",
        "  frequencies near f_notch. A high Q will make the notch filter more precise, \n",
        "  but it will not attenuate f_notch as much. \n",
        "\"\"\"\n",
        "fc_passband = [75,115]\n",
        "order = 4\n",
        "applyNotch = True \n",
        "f_notch = [60,120]; \n",
        "Q = 50\n",
        "\n",
        "def apply_filter(raw_signal):\n",
        "  \"\"\"\n",
        "  Input: \n",
        "    raw_signal (samples x channels): the raw signal\n",
        "  Output: \n",
        "    clean_data (samples x channels): the filtered signal\n",
        "  \"\"\"\n",
        "  number_of_channels = np.shape(raw_signal)[1] #number of channels\n",
        "  filteredData = np.zeros(np.shape(raw_signal)); #filtered data output\n",
        "\n",
        "  # Bandpass Butterworth filter \n",
        "  sos = sig.butter(order, fc_passband, 'bandpass', analog=False, fs=fs, output='sos'); # returns filter coefficients\n",
        "  b_notch = []; a_notch = []\n",
        "  for f_remove in f_notch:\n",
        "    b, a = sig.iirnotch(f_remove,Q,fs=fs)\n",
        "    b_notch.append(b); a_notch.append(a)\n",
        "  #for each channel\n",
        "  for chanInd in np.arange(number_of_channels):\n",
        "    # subtract mean from each datapoint\n",
        "    currFilt = raw_signal[:, chanInd] - np.mean(raw_signal[:, chanInd]);\n",
        "    if(applyNotch): \n",
        "      for i in range(len(b_notch)):\n",
        "        currFilt = sig.filtfilt(b_notch[i],a_notch[i],currFilt)\n",
        "    currFilt = sig.sosfiltfilt(sos, currFilt) # forward-backward digital filter using cascaded second-order sections                                        \n",
        "    filteredData[:, chanInd] = currFilt\n",
        "  return filteredData"
      ],
      "metadata": {
        "id": "PV8onTEvpNyu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction"
      ],
      "metadata": {
        "id": "Lsp4l6bZowqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Class\n",
        "Every feature is a child of this class. This class streamlines the process of adding features and how we take extract them from the ECoG data. We can easily change whether a feature is normalized (using standardization method) and whether it uses the raw or filtered ECoG data. \n",
        "This class will save the mean and standard deviation of the training data so it can easily standardize both training and testing data.\n",
        "\n",
        "We can define every feature we consider here, but the only features that are extracted and used in the models will be the ones we instantiate as objects. Every object we instantiate is automatically added to the global list *featFns*, which keeps track of the features we are analyzing. "
      ],
      "metadata": {
        "id": "t1Q4qzPX3CPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global featFns;\n",
        "featFns = [] # A list that stores all the Feature Objects used\n",
        "featDict = dict() # a dict that stores each Feature Object. \n",
        "  # The dict keys are from its get_name method.\n",
        "\n",
        "class Feature():\n",
        "  def __init__(self,isFiltered=True, doNormalize=True):\n",
        "    self.isFiltered = isFiltered # Whether or not the feature is extracted from the raw or filtered data.\n",
        "    self.doNormalize = doNormalize # Whether or not to normalize this feature\n",
        "    self.mean = 0 # mean\n",
        "    self.std = 1 # standard deviation\n",
        "    featFns.append(self)\n",
        "\n",
        "  def __call__(self, signal_data):\n",
        "    return signal_data\n",
        "\n",
        "  def get_name(self,nameAddon=None):\n",
        "    name = type(self).__name__\n",
        "    if nameAddon is not None:\n",
        "      name += nameAddon\n",
        "    name_filter = 'Filtered'\n",
        "    name_norm = 'Normalized'\n",
        "    if not self.isFiltered: name_filter = 'not' + name_filter\n",
        "    if not self.doNormalize: name_norm = 'not' + name_norm\n",
        "    fullname = name + '_' + name_filter + '_' + name_norm\n",
        "    featDict[fullname] = self\n",
        "    return fullname\n",
        "\n",
        "  def standardize_training(self,training_feat):\n",
        "    if(self.doNormalize):\n",
        "      self.mean = np.mean(training_feat)\n",
        "      self.std = np.std(training_feat)\n",
        "    return (training_feat - self.mean)/self.std\n",
        "\n",
        "  def standardize_testing(self,testing_feat):\n",
        "    return (testing_feat-self.mean) / self.std\n",
        "  \n"
      ],
      "metadata": {
        "id": "zaJSPNcPpEGn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Definitions\n",
        "This is where the functions for the different features are defined. The functions themselves should be written in the __ call__(self, window) method.\n",
        "\n",
        "Note that the functions are applied to one window in one channel at a time. "
      ],
      "metadata": {
        "id": "kbCjx0dXHqKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LineLength(Feature):\n",
        "  def __call__(self,x):\n",
        "    return np.sum(np.absolute(np.ediff1d(x)))\n",
        "\n",
        "class Area(Feature):\n",
        "  def __call__(self,x):\n",
        "    return np.sum(np.absolute(x))\n",
        "\n",
        "class Energy(Feature):\n",
        "  def __call__(self,x):\n",
        "    return np.sum(np.square(x))\n",
        "\n",
        "class ZeroCrossings(Feature):\n",
        "  def __call__(self,x):\n",
        "    return np.size(np.nonzero(np.ediff1d(np.sign(x-np.mean(x)))))\n",
        "\n",
        "class Mean(Feature):\n",
        "  def __call__(self,x):\n",
        "    return np.mean(x)\n",
        "\n",
        "class FreqBand(Feature):\n",
        "  def __init__(self,f_low,f_high,isFiltered=False,doNormalize=True):\n",
        "    Feature.__init__(self,isFiltered,doNormalize)\n",
        "    self.f_low = f_low\n",
        "    self.f_high = f_high\n",
        "  \n",
        "  def __call__(self,signal):\n",
        "    freq_response = fft(signal)\n",
        "    N = len(freq_response)\n",
        "    n = np.arange(N)\n",
        "    T = N/fs #sampling rate=1000\n",
        "    freq = n/T \n",
        "    power_spectrum = np.abs(freq_response)\n",
        "    # Find values in frequency vector corresponding to input band\n",
        "    index_band = np.logical_and(freq >= self.f_low, freq <= self.f_high)\n",
        "    #average frequency domain magnitude\n",
        "    avg_mag = np.mean(power_spectrum[index_band])\n",
        "    return avg_mag\n",
        "\n",
        "  def get_name(self):\n",
        "    nameAddon = str(self.f_low) + 'to' + str(self.f_high)\n",
        "    return Feature.get_name(self,nameAddon)"
      ],
      "metadata": {
        "id": "59khQQ-oHq8M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Windowed Feats"
      ],
      "metadata": {
        "id": "nS6g6VTP4Ry2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NumWins(x,winLen,winDisp,fs=1000):\n",
        "  \"\"\"\n",
        "    Calculates the number of possible full windows that can fit in x\n",
        "    Inputs:\n",
        "      x is the signal in the time domain. \n",
        "      fs is the sampling frequency of x. Hz.\n",
        "      winLen is the length of windows. sec\n",
        "      winDisp is the displacement between the start of each window. sec\n",
        "  \"\"\"\n",
        "  x_duration = len(x)/fs # seconds.\n",
        "  windows_fit = (x_duration - winLen + winDisp) / (winDisp)\n",
        "  # default behaviour of int() is to floor float, so using round()\n",
        "  return round(windows_fit)\n",
        "\n",
        "def get_features(raw_window,filtered_window):\n",
        "  \"\"\"\n",
        "    Input: \n",
        "      raw_window (window_samples x channels): the window of the unfiltered ecog signal \n",
        "      filtered_window (window_samples x channels): the window of the filtered ecog signal \n",
        "      \n",
        "\n",
        "    Global Inputs: must be defined outside of the function\n",
        "      featFns: a list containing the methods to apply as feats. \n",
        "    \n",
        "    Output:s\n",
        "      features (channels x num_features): the features calculated on each channel for the window\n",
        "  \"\"\"\n",
        "  [window_samples,num_channels]=np.shape(raw_window)\n",
        "  features = np.empty(num_channels*len(featFns))\n",
        "  i = 0\n",
        "  for feat in featFns:\n",
        "    if feat.isFiltered: window = filtered_window\n",
        "    else: window = raw_window\n",
        "    for chn in range(num_channels):\n",
        "      current_window = window[:,chn]\n",
        "      features[num_channels*i+chn] = feat(current_window)\n",
        "    i+=1\n",
        "  return features\n",
        "\n",
        "def get_windowed_feats(ecog_data, window_length, window_overlap):\n",
        "  \"\"\"\n",
        "    Inputs:\n",
        "      raw_eeg (samples x channels): the raw signal\n",
        "      window_length: the window's length\n",
        "      window_overlap: the window's overlap\n",
        "    Output: \n",
        "      all_feats (num_windows x (channels x features)): the features for each channel for each time window\n",
        "        note that this is a 2D array. \n",
        "  \"\"\"\n",
        "  [num_samples,num_channels]=np.shape(ecog_data)\n",
        "  num_windows = NumWins(ecog_data, window_length,window_overlap, fs) \n",
        "  filtered_ecog = apply_filter(ecog_data)\n",
        "  #convert everything to units of samples\n",
        "  wLen=round(window_length*fs) #window length in samples\n",
        "  wDisp=round(window_overlap*fs) #window displacement in samples\n",
        "  data_feats = np.zeros((num_windows,num_channels*len(featFns))); # stores the features of the window\n",
        "   \n",
        "  rightmost = num_samples\n",
        "  for i in range(num_windows):\n",
        "    raw_window = ecog_data[rightmost-wLen:rightmost,:]\n",
        "    filtered_window = filtered_ecog[rightmost-wLen:rightmost,:]\n",
        "    data_feats[-1-i,:] = (get_features(raw_window,filtered_window).flatten())\n",
        "    rightmost = rightmost - wDisp\n",
        "  return data_feats\n"
      ],
      "metadata": {
        "id": "eg6MkCDg5Csq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Normalization\n",
        "We are using standardization to normalize every feature.\n",
        ":"
      ],
      "metadata": {
        "id": "WP_6_JBw4Xd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_training(feature_matrix):\n",
        "  [windows_trn, feats_trn] = np.shape(feature_matrix)\n",
        "  numChns = int(feats_trn/len(featFns))\n",
        "  normFeats = np.empty((windows_trn,feats_trn))\n",
        "  for i in range(len(featFns)):\n",
        "    for j in range(numChns):\n",
        "      column = i*numChns + j\n",
        "      normFeats[:,column] = featFns[i].standardize_training(feature_matrix[:,column])\n",
        "  return normFeats\n",
        "\n",
        "def standardize_testing(feature_matrix):\n",
        "  [windows_trn,feats_trn] = np.shape(feature_matrix)\n",
        "  numChns = int(feats_trn/len(featFns))\n",
        "  normFeats = np.empty((windows_trn,feats_trn))\n",
        "  for i in range(len(featFns)):\n",
        "    for j in range(numChns):\n",
        "      column = i*numChns + j\n",
        "      normFeats[:,column] = featFns[i].standardize_testing(feature_matrix[:,column])\n",
        "  return normFeats\n",
        "\n",
        "def standardize_both(train_feats, test_feats):\n",
        "  normed_trn = standardize_training(train_feats)\n",
        "  normed_tst = standardize_testing(test_feats)\n",
        "  return normed_trn, normed_tst"
      ],
      "metadata": {
        "id": "S-ntPeP04bOd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Response Matrix\n",
        "We will be using a response matrix as the input to each of our learning algorithms. This is because it allows us to associate one window with the *N_wind* windows before it instead of treating each window as an independent case.   "
      ],
      "metadata": {
        "id": "m_qSn_Au4uE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_R_matrix(features, N_wind):\n",
        "  \"\"\" \n",
        "  Input:\n",
        "    features (samples (number of windows in the signal) x channels x features): \n",
        "      the features you calculated using get_windowed_feats\n",
        "    N_wind: number of windows to use in the R matrix\n",
        "\n",
        "  Output:\n",
        "    R (samples x (N_wind*channels*features))\n",
        "  \"\"\"\n",
        "  features_appended = np.copy(features)\n",
        "  for i in list(range(N_wind-2, -1, -1)):\n",
        "      a = features[i]\n",
        "      features_appended = np.vstack([a, features_appended])\n",
        "  samples = len(features)   # number of rows = number of windows\n",
        "\n",
        "  R = np.zeros((samples, 1+(N_wind*len(features[0,:]))))  # len(features[0,:]) = (num of features)*(num of channels)\n",
        "  lst = np.array(list(range(1, 1+N_wind)))\n",
        "  R[:, 0] = 1\n",
        "  \n",
        "  \n",
        "  for i in range(len(features[0,:])):   # goes thru each column of the features matrix\n",
        "    for j in range(len(lst)):\n",
        "        x = lst[j]\n",
        "        R[:, x] = features_appended[j : (len(features_appended)-(N_wind-1-j)), i]\n",
        "    lst = lst + N_wind\n",
        "  return R\n"
      ],
      "metadata": {
        "id": "Y8d0IL7L43EH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downsampling The Glove Data\n",
        "We find the features for each window in the ECoG data, and this is how we decide the value for the glove data in the same windows. The downsample methods defined below are analagous to feats, but applied to the windows in glove data instead of ECoG. \n",
        "\n",
        "Below are several functions used to downsample the data. Simple downsampling would just be taking the glove data at the startpoint or endpoint of the window. However, changing our downsampling method may increase our correlation coefficient. Several methods are defined below and the one that we use is defined later in the Define Paramters section."
      ],
      "metadata": {
        "id": "_qOYt8uy1nRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def startpoint_downsample(finger_window):\n",
        "  return finger_window[0]\n",
        "\n",
        "def endpoint_downsample(finger_window):\n",
        "  return finger_window[-1]\n",
        "\n",
        "def max_downsample(finger_window):\n",
        "  return np.max(finger_window)\n",
        "\n",
        "def area_downsample(finger_window):\n",
        "  return np.sum(np.absolute(finger_window))"
      ],
      "metadata": {
        "id": "XhtUhQUU2LQr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample_window(glove_window, downsample_method):\n",
        "  \"\"\"\n",
        "    This function applies the chosen downsampling method to a window of the \n",
        "    glove. Returns a downsampled value for each channel (each finger). \n",
        "  \"\"\"\n",
        "  [num_samples,num_channels] = np.shape(glove_window)\n",
        "  downsampled_glove = np.empty(num_channels)\n",
        "  for chn in range(num_channels):\n",
        "    current_window = glove_window[:,chn]\n",
        "    downsampled_glove[chn] = downsample_method(current_window)\n",
        "  return downsampled_glove\n",
        "\n",
        "def downsample(glove_data, winLen, winDisp, downsample_method):\n",
        "  \"\"\"\n",
        "    Creates an array of the downsampled glove data for each channel (finger).\n",
        "    Inputs:\n",
        "      glove_data = the glove data for finger flexion. Should have 5 channels.\n",
        "\n",
        "    Returns: an array of size (windows x glove channels) that represents the \n",
        "      downsampled glove data for each window. \n",
        "  \"\"\"\n",
        "  [num_samples,num_channels] = np.shape(glove_data)\n",
        "  num_windows = NumWins(glove_data, winLen,winDisp, fs)\n",
        "  wLen = round(winLen*fs) #window length in samples\n",
        "  wDisp = round(winDisp*fs) #window displacement in samples\n",
        "  windowed_fingers = np.zeros((num_windows,num_channels)); # stores the features of the window\n",
        "  rightmost = num_samples\n",
        "  for i in range(num_windows):\n",
        "    window = glove_data[rightmost-wLen:rightmost,:]\n",
        "    windowed_fingers[-1-i,:] = (downsample_window(window, downsample_method).flatten())\n",
        "    rightmost = rightmost - wDisp\n",
        "  return windowed_fingers\n"
      ],
      "metadata": {
        "id": "4oOf9H7D7VrY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Parameters"
      ],
      "metadata": {
        "id": "ZCGpfarf6r8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User-Defined Parameters\n",
        "Below are the user defined parameters. All the parameters are defined here, except for the ones that have already been defined for the filter in the Filter Design section."
      ],
      "metadata": {
        "id": "AYi9mZrltChU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of the file to write or read the data from for parameters and R matrices\n",
        "filename = 'previousRun.pkl'\n",
        "use_file_params = False \n",
        "# If use_file_params is True, then it will overwrite the parameters defined here\n",
        "\n",
        "# Window Parameters\n",
        "window_length = 100e-3  # seconds\n",
        "window_displacement = 50e-3 #seconds\n",
        "N_winds = 3 # Number of previous windows considered in Response matrix. \n",
        "\n",
        "# These Booleans are to make code clearer\n",
        "uses_raw = False; uses_filtered = True \n",
        "not_normalized = False; normalized = True; \n",
        "# Different Features analyzed\n",
        "featFns=[] # Empty list of featFns before defining feats for this runtime.\n",
        "# line_length = LineLength(uses_filtered,normalized)\n",
        "# area = Area(uses_filtered,normalized)\n",
        "# zero_crossings = ZeroCrossings(uses_filtered,normalized)\n",
        "energy = Energy(uses_filtered,normalized)\n",
        "mean = Mean(uses_filtered,normalized)\n",
        "# freq_band_5_to_15 = FreqBand(5,15,uses_raw,normalized)\n",
        "# freq_band_20_to_25 = FreqBand(20,25,uses_raw,normalized)\n",
        "# freq_band_75_to_115 = FreqBand(75,115,uses_raw,normalized)\n",
        "# freq_band_125_to_160 = FreqBand(125,160,uses_raw,normalized)\n",
        "# freq_band_160_to_175 = FreqBand(160,175,uses_raw,normalized)\n",
        "freq_band_8_to_12 = FreqBand(8,12,uses_raw,normalized)\n",
        "freq_band_75_to_95 = FreqBand(75,95,uses_raw,normalized)\n",
        "freq_band_96_to_115 = FreqBand(96,115,uses_raw,normalized)\n",
        "# Training / Testing Split\n",
        "training_fraction = 2/3 # What fraction of the samples\n",
        "\n",
        "# Downsampling method for finger flexion in glove data.\n",
        "downsample_method = startpoint_downsample\n"
      ],
      "metadata": {
        "id": "3xT8HfGK6vsD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Dictionary\n",
        "Here we put all the parameters into a dictionary. This makes it easy to export them and compare them between trials. This dictionary will be compared to parameter dictionaries from previous trials, and if they use the same parameters, we will just load the previous feature and R matrices instead of calculating them all again. The saving and loading code is towards the end of the Preparing Data section."
      ],
      "metadata": {
        "id": "79zFtNHr0Gtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global param_dict;\n",
        "param_dict = dict()\n",
        "# Filter Params\n",
        "param_dict['fs'] = fs\n",
        "param_dict['fc_passband'] = fc_passband \n",
        "param_dict['order'] = order\n",
        "param_dict['applyNotch'] = applyNotch\n",
        "param_dict['f_notch'] = f_notch\n",
        "param_dict['Q'] = Q\n",
        "# Window Parameters\n",
        "param_dict['winLen'] = window_length\n",
        "param_dict['winDisp'] = window_displacement\n",
        "param_dict['N_winds'] = N_winds\n",
        "# Feature Parameters\n",
        "param_dict['featFns'] = []\n",
        "for feat in featFns:\n",
        "  param_dict['featFns'].append(feat.get_name())\n",
        "# Training / Testing Split Parameter\n",
        "param_dict['training_fraction'] = training_fraction\n",
        "\n",
        "# Downsampling method for finger flexion in glove data.\n",
        "param_dict['downsampling'] = downsample_method.__name__\n"
      ],
      "metadata": {
        "id": "JGge8-zh0jhQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "Putting the data in a form that can be easily used for different learning algorithms. "
      ],
      "metadata": {
        "id": "lEp6hxF9ozMl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Functions\n",
        "These are functions for splitting the data into training and testing for validation purposes. Additionally, functions to save, load, and compare data files are defined here."
      ],
      "metadata": {
        "id": "jBIEoe-8z1m8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, train_fraction = training_fraction):\n",
        "  \"\"\"\n",
        "    Inputs:\n",
        "    data = a samples x channels array of data for one patient\n",
        "    training_fraction = a number between 0 and 1 that represents the fraction of\n",
        "      data that will be put into the training split. The remaining will be put\n",
        "      into the testing split. \n",
        "    Returns:\n",
        "      training data, testing data\n",
        "  \"\"\"\n",
        "  m = len(data[:,0]) # Number of samples per channel\n",
        "  m_training = round(train_fraction*m)\n",
        "  training_data = data[0:m_training,:]\n",
        "  testing_data = data[m_training:m,:]\n",
        "  return training_data, testing_data\n",
        "\n",
        "def save_feature_parameters(filename):  \n",
        "  with open(filename, 'wb') as f:\n",
        "    # Note that this will overwrite any data already in filename\n",
        "    pickle.dump(param_dict,f)\n",
        "  print(f\"The parameters and R matrices have been saved in {filename}\")\n",
        "\n",
        "def load_feature_parameters(filename):\n",
        "  with open(filename, 'rb') as f:\n",
        "    ref_dict = pickle.load(f)\n",
        "  return ref_dict\n",
        "\n",
        "def compare_dicts(dict1, dict2):\n",
        "  \"\"\"\n",
        "    Input: dict1 and dict2 are borth dictionaries. The expectation is that they\n",
        "      have the same keys. \n",
        "    Returns: the number of differences between the keys in dict1 and the keys in\n",
        "      dict2. It will also print them out.  \n",
        "  \"\"\"\n",
        "  differences = DeepDiff(dict1,dict2)\n",
        "  num_differences = len(differences.to_dict())\n",
        "  if num_differences > 0:\n",
        "    print(f'{filename} uses different parameters from the ones currently set.')\n",
        "    print(differences)\n",
        "  return num_differences"
      ],
      "metadata": {
        "id": "FrE3qg7l0w7N"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "3ugGr1xSzUbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where all the R Matrices are either loaded or created depending on whether the filename provided has them already. If it does not, then the newly created R matrices will be saved into it. "
      ],
      "metadata": {
        "id": "-z3QgG4EToOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_ecog = proj_data['train_ecog'][:,0]\n",
        "raw_glove = proj_data['train_dg'][:,0]\n",
        "raw_leaderboard = leaderboard_data['leaderboard_ecog'][:,0]"
      ],
      "metadata": {
        "id": "3ec3JILB1P58"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newDataNeeded = True \n",
        "# new Data Needed is True by default, but set to False if data matching the\n",
        "# parameters described in param_dict are found in filename. \n",
        "\n",
        "try: # This Try-Except block is in case filename does not exist\n",
        "  ref_dict = load_feature_parameters(filename)\n",
        "  ref_dict_backup = ref_dict.copy() # backup because popping removes from dict.\n",
        "  # This will extract the preprocessed data from the file\n",
        "  # The first time running the code, param_dict will not contain these keys, so\n",
        "  # popping them makes the ref_dict more comparable to param_dict.  \n",
        "  R_total = ref_dict.pop('R_total',None)\n",
        "  R_train = ref_dict.pop('R_train',None)\n",
        "  R_test = ref_dict.pop('R_test',None)\n",
        "  R_leaderboard = ref_dict.pop('R_leaderboard',None) \n",
        "  flexion_total = ref_dict.pop('flexion_total',None)\n",
        "  flexion_train = ref_dict.pop('flexion_train',None)\n",
        "  flexion_test = ref_dict.pop('flexion_test',None)\n",
        "  \n",
        "  # After running the code for the first time, param_dict might contain keys for\n",
        "  # R matrices, so this allows code to work without needing to restart runtime. \n",
        "  if 'R_leaderboard' in param_dict:\n",
        "    ref_dict = ref_dict_backup\n",
        "\n",
        "  # This compares the user defined parameters to the ones from the file. \n",
        "  if compare_dicts(param_dict, ref_dict) == 0:\n",
        "    newDataNeeded = False\n",
        "    print(f\"{filename} uses the same parameters as currently set, so it will be loaded\")\n",
        "  \n",
        "  # uses parameters from file instead of the user defined ones.\n",
        "  elif use_file_params:\n",
        "    newDataNeeded = False\n",
        "    print(f\"{filename} uses different parameters, but use_file_params is True\")\n",
        "except: \n",
        "  print(f'{filename} was not found. Creating new a dataset instead')\n",
        "finally:\n",
        "  if newDataNeeded:\n",
        "    flexion_total = []; flexion_train = []; flexion_test = []\n",
        "    R_total = []; R_leaderboard = []; R_train = []; R_test = []\n",
        "    for p in range(numPatients):\n",
        "      # Creates the R matrix for all of the raw ecog data\n",
        "      feat_matrix_p = get_windowed_feats(raw_ecog[p], window_length, window_displacement)\n",
        "      normed_feat_matrix_p = standardize_training(feat_matrix_p)\n",
        "      R_total.append(create_R_matrix(normed_feat_matrix_p,N_winds))\n",
        "      \n",
        "      # Calculates the R matrix of the leaderboard ecog data. \n",
        "      leaderboard_p = get_windowed_feats(raw_leaderboard[p], window_length, window_displacement)\n",
        "      normed_leaderboard_p = standardize_testing(leaderboard_p)\n",
        "      R_leaderboard.append(create_R_matrix(normed_leaderboard_p,N_winds))\n",
        "\n",
        "      # Creates the R matrix for the training and testing split of the ecog data\n",
        "      trn_matrix_p, tst_matrix_p = split_data(feat_matrix_p)\n",
        "      normed_trn_matrix_p, normed_tst_matrix_p = standardize_both(trn_matrix_p, tst_matrix_p)\n",
        "      R_train.append(create_R_matrix(normed_trn_matrix_p,N_winds))\n",
        "      R_test.append(create_R_matrix(normed_tst_matrix_p,N_winds))\n",
        "\n",
        "      # Creates the downsampled glove data for the entire raw ecog dataset, and\n",
        "      # the training and testing split. \n",
        "      flexion_p = downsample(raw_glove[p], window_length, window_displacement, downsample_method)\n",
        "      flexion_total.append(flexion_p)\n",
        "      trn_flexion_p, tst_flexion_p = split_data(flexion_p)\n",
        "      flexion_train.append(trn_flexion_p); flexion_test.append(tst_flexion_p)\n",
        "    \n",
        "    # Store the resulting matrices into param_dict and save it to filename\n",
        "    param_dict['flexion_total'] = flexion_total\n",
        "    param_dict['flexion_train'] = flexion_train\n",
        "    param_dict['flexion_test'] = flexion_test\n",
        "    param_dict['R_total'] = R_total\n",
        "    param_dict['R_train'] = R_train\n",
        "    param_dict['R_test'] = R_test\n",
        "    param_dict['R_leaderboard'] = R_leaderboard\n",
        "    save_feature_parameters(filename)\n"
      ],
      "metadata": {
        "id": "TYPDWndiUaZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf425a2-848a-49b3-a778-47b04fb4a6f7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "previousRun.pkl uses the same parameters as currently set, so it will be loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Algorithms"
      ],
      "metadata": {
        "id": "AT3zJxIco_va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Helper Classes\n",
        "Here two classes are defined to streamline the process of adding new learning algorithms. The LearningModel class is the parent class of every type of model we will use. The LearningAlgo class is a helper class that allows us to apply a LearningModel on each finger seperately while keeping track of the data.\n",
        "\n",
        "The only methods that might need changed to implement subclasses are the train, predict, clone, and maybe __ init __, methods in LearningModel. \n",
        "\n",
        "Note that LearningModel is designed so that it can work directly with most sklearn models without needing to create a subclass. "
      ],
      "metadata": {
        "id": "sVqFAVb4VFw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LearningModel():\n",
        "  def __init__(self, model_type = None, smoothing = None):\n",
        "    \"\"\"\n",
        "      model_type is the unfit model being used.\n",
        "      smoothing is a function that applies smoothing to the prediction that\n",
        "        right now it does nothing because I don't know how to implement it.\n",
        "    \"\"\"\n",
        "    self.model_type = model_type\n",
        "    self.smoothing = smoothing\n",
        "\n",
        "  def train(self, R_trn, y_trn):\n",
        "    \"\"\"\n",
        "      This method trains a single model on all of y_trn. This method will need\n",
        "      redefined in subcasses if self.model_type.fit(data,labels) does not exist \n",
        "      or if self.model_type is not from the sklearn library. \n",
        "      Inputs:\n",
        "        R_trn = Response matrix for the training data set used\n",
        "        y_trn = finger flexion matrix for the training data set used.\n",
        "      Returns:\n",
        "        A model trained on R_trn and y_trn.\n",
        "    \"\"\"\n",
        "    self.model = sklearn.base.clone(self.model_type)\n",
        "    self.model.fit(R_trn, y_trn)\n",
        "    return self.model\n",
        "  \n",
        "  def predict(self, R_tst):\n",
        "    \"\"\"\n",
        "      This method predicts a y matrix based on the training it has recieved.\n",
        "      This method will fail if train has not been called yet. This, method will\n",
        "      need redefined in subcasses if self.model_type.predict(data) does not \n",
        "      exist or if self.model_type is not from the sklearn library. \n",
        "      Inputs:\n",
        "        R_tst = Response matrix for the training data set used\n",
        "      Returns:\n",
        "        a y matrix predicted to be the labels associated with R_tst\n",
        "    \"\"\"\n",
        "    self.prediction = self.model.predict(R_tst)\n",
        "    return self.prediction\n",
        "  \n",
        "  def score(self, R_tst, Y_tst):\n",
        "    \"\"\"\n",
        "      Predicts the Y for R_tst and then compares it to the actual Y_tst.\n",
        "      Inputs:\n",
        "        R_tst = response matrix for testing or validation data set.\n",
        "        Y_tst = finger flexion matrix for the testing data set used.\n",
        "    \"\"\"\n",
        "    Y_predict = self.predict(R_tst)\n",
        "    self.correlation = pearsonr(Y_predict,Y_tst).statistic\n",
        "    return self.correlation\n",
        "\n",
        "  def clone(self):\n",
        "    return LearningModel(self.model_type, self.smoothing)\n",
        "\n",
        "\n",
        "class LearningAlgo():\n",
        "  \"\"\"\n",
        "    This helper class will train multiple of the same type of learning channel\n",
        "    on different parts of the data. In particular, it will train a model on \n",
        "    each channel within the label matrix Y_trn for each patient.\n",
        "\n",
        "    Unlike LearningModel, subclasses of LearningAlgo should not need to be made\n",
        "    for implementing new learning models. It should work for any model that is\n",
        "    a subclass of the LearningModel class.\n",
        "\n",
        "    It stores its models in a nested list self.models. To get the model of \n",
        "    finger i for patient p, use self.models[p][i]\n",
        "  \"\"\"\n",
        "  def __init__(self, learning_model):\n",
        "    \"\"\"\n",
        "      Its only input should be a LearningModel (or subclass) object \n",
        "    \"\"\"\n",
        "    self.learning_model = learning_model\n",
        "    self.model_type = learning_model.model_type\n",
        "    self.smoothing = learning_model.smoothing\n",
        "    self.models = [[]]\n",
        "\n",
        "  def train(self, R_trn, Y_trn):\n",
        "      \"\"\"\n",
        "        Will train a model on each finger in Y_trn. The total number of models \n",
        "        will be equal to channels in Y_trn \n",
        "        Inputs:\n",
        "          R_trn = A list of Response matrices for the training data set used\n",
        "          Y_trn = a list of finger flexion matrices for the training data set \n",
        "            used. Each element should have finger flexion for each window for\n",
        "            all five fingers. \n",
        "          The list of R_trn and Y_trn matrices should be the same length\n",
        "        Returns:\n",
        "          The list of models trained. The models at each index, i, corresponds \n",
        "          to the model trained on finger i+1\n",
        "      \"\"\"\n",
        "      self.models = []\n",
        "      for p in range(len(R_trn)):\n",
        "        models_p = []\n",
        "        for finger in range(len(Y_trn[p][0,:])):\n",
        "          model = self.learning_model.clone()\n",
        "          model.train(R_trn[p], Y_trn[p][:,finger])\n",
        "          models_p.append(model)\n",
        "        self.models.append(models_p)\n",
        "      return self.models\n",
        "\n",
        "  def predict(self,R_tst):\n",
        "    \"\"\"\n",
        "      This method predicts a Y matrix based on the training it has recieved.\n",
        "      This method will fail if train has not been called yet. \n",
        "      Inputs:\n",
        "        R_tst = Response matrix for the training data set used\n",
        "      Returns:\n",
        "        a Y matrix predicted to be the labels associated with R_tst\n",
        "    \"\"\"\n",
        "    predictions = np.zeros((len(self.models),len(self.models[0])))\n",
        "    for p in range(len(self.models)):\n",
        "      for i in range(len(self.models[p])):\n",
        "        predictions[p,i] = self.models[p][i].predict(R_tst[p])\n",
        "    return predictions\n",
        "  \n",
        "  def score(self, R_tst, Y_tst):\n",
        "    \"\"\"\n",
        "      Predicts the Y for R_tst and then compares it to the actual Y_tst.\n",
        "      Inputs:\n",
        "        R_tst = response matrix for testing or validation data set.\n",
        "        Y_tst = finger flexion matrix for the testing data set used.\n",
        "    \"\"\"\n",
        "    correlations = np.zeros((len(self.models),len(self.models[0])))\n",
        "    for p in range(len(self.models)):\n",
        "      for i in range(len(self.models[p])):\n",
        "        correlations[p,i] = self.models[p][i].score(R_tst[p],Y_tst[p][:,i])\n",
        "    return correlations\n",
        "  \n",
        "  def get_models(self):\n",
        "    return self.models\n",
        "\n",
        "  def print_scores(self):\n",
        "    for p in range(len(self.models)):\n",
        "      for i in range(len(self.models[p])):\n",
        "        print(f\"Patient {p+1}, Finger {i+1} Score = {self.models[p][i].correlation}\")\n"
      ],
      "metadata": {
        "id": "xTT7TIaJJlXW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimized Linear Filter"
      ],
      "metadata": {
        "id": "9HnI3yAWzkJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizedLinearFilter(LearningModel):\n",
        "  def train(self, R, Y):\n",
        "    \"\"\"\n",
        "      Builds an optimized linear filter using a training R and Y\n",
        "      Inputs: \n",
        "        R = Response matrix for the training data set used\n",
        "        Y = label matrix (finger flexion) for the training data set used\n",
        "      Returns: an optimized linear filter. \n",
        "    \"\"\"\n",
        "    Rt = np.transpose(R)\n",
        "    inv_term = inv(np.matmul(Rt,R))\n",
        "    self.model_type = 'Optimized Linear Filter' \n",
        "    self.model = np.matmul(inv_term,np.matmul(Rt,Y))\n",
        "    return self.model\n",
        "\n",
        "  def predict(self, R_tst):\n",
        "    self.prediction = np.matmul(R_tst,self.model)\n",
        "    return self.prediction\n",
        "\n",
        "  def clone(self):\n",
        "    return OptimizedLinearFilter(self.model_type, self.smoothing)\n",
        "\n",
        "opt_linear_filters_val_algo = LearningAlgo(OptimizedLinearFilter())\n",
        "opt_linear_filters_val_algo.train(R_train, flexion_train)\n",
        "optlinfilt_val_score = opt_linear_filters_val_algo.score(R_test,flexion_test)\n",
        "# opt_linear_filters_val = [] # for validation \n",
        "# opt_linear_filters_lead = [] # for the leaderboard\n",
        "# for p in range(numPatients):\n",
        "#  opt_linear_filters_val.append(LearningAlgo(OptimizedLinearFilter())) \n",
        "#  opt_linear_filters_val[p].train(R_train,flexion_train)\n",
        "#  opt_linear_filters_lead.append(LearningAlgo(OptimizedLinearFilter()))\n",
        "print(optlinfilt_val_score[0])\n",
        "print(optlinfilt_val_score[1])\n",
        "print(optlinfilt_val_score[2])\n"
      ],
      "metadata": {
        "id": "qkP8WX_cpCuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f813bcd-6f67-406f-e579-99ecffa22b92"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35495081 0.44099169 0.04711173 0.37991786 0.06742433]\n",
            "[0.10627965 0.1564085  0.14984541 0.22359355 0.06892738]\n",
            "[0.54612488 0.37248241 0.40417065 0.42448121 0.41380011]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVR Model"
      ],
      "metadata": {
        "id": "S76MYJLtFDws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "SVR_model = LearningModel(SVR(kernel = 'rbf'))\n",
        "SVR_algo = LearningAlgo(SVR_model)\n",
        "trained_models = SVR_algo.train(R_train,flexion_train)\n",
        "correlation_scores = SVR_algo.score(R_test,flexion_test)\n",
        "print(correlation_scores[0])\n",
        "print(correlation_scores[1])\n",
        "print(correlation_scores[2])"
      ],
      "metadata": {
        "id": "O3cMV4sLFETI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e090fe1d-0a6f-4124-d605-5df1a30a38f8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35261238 0.44150977 0.06083909 0.42818931 0.13588042]\n",
            "[-0.02465233  0.00715963 -0.03202354  0.01230133 -0.01379379]\n",
            "[0.5181644  0.40935245 0.40859364 0.45392429 0.42303565]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SVR_algo.print_scores()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyLLqJ_FGarL",
        "outputId": "c814e21c-4f4c-4989-ca0c-7b99fd6e836b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient 1, Finger 1 Score = 0.35261237563361925\n",
            "Patient 1, Finger 2 Score = 0.4415097669134386\n",
            "Patient 1, Finger 3 Score = 0.06083909416828889\n",
            "Patient 1, Finger 4 Score = 0.42818931148001327\n",
            "Patient 1, Finger 5 Score = 0.13588042015407736\n",
            "Patient 2, Finger 1 Score = -0.024652331108691382\n",
            "Patient 2, Finger 2 Score = 0.007159625618124479\n",
            "Patient 2, Finger 3 Score = -0.03202353819260955\n",
            "Patient 2, Finger 4 Score = 0.012301329981778266\n",
            "Patient 2, Finger 5 Score = -0.013793793297055865\n",
            "Patient 3, Finger 1 Score = 0.5181643997631837\n",
            "Patient 3, Finger 2 Score = 0.4093524525545632\n",
            "Patient 3, Finger 3 Score = 0.4085936425615513\n",
            "Patient 3, Finger 4 Score = 0.4539242899456535\n",
            "Patient 3, Finger 5 Score = 0.4230356541032878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(np.mean(predictions - flexion_test[p][:,f]))"
      ],
      "metadata": {
        "id": "SeBIyAgaKaES"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scores = selected.score(R_test[p], flexion_test[p][:,f])\n",
        "# print(scores)"
      ],
      "metadata": {
        "id": "p3VVqD62ivM7"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}